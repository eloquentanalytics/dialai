"use strict";(globalThis.webpackChunkdialai_website=globalThis.webpackChunkdialai_website||[]).push([[8153],{1432(e,n,i){i.r(n),i.d(n,{assets:()=>h,contentTitle:()=>l,default:()=>d,frontMatter:()=>r,metadata:()=>t,toc:()=>o});const t=JSON.parse('{"id":"concepts/human-primacy","title":"Human Primacy","description":"The human is always right \u2014 not because humans are infallible, but because humans have context that AI cannot access.","source":"@site/docs/concepts/human-primacy.md","sourceDirName":"concepts","slug":"/concepts/human-primacy","permalink":"/dialai/docs/concepts/human-primacy","draft":false,"unlisted":false,"editUrl":"https://github.com/eloquentanalytics/dialai/tree/main/website/docs/concepts/human-primacy.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Arbitration","permalink":"/dialai/docs/concepts/arbitration"},"next":{"title":"State Machines","permalink":"/dialai/docs/guides/state-machines"}}');var s=i(4848),a=i(8453);const r={sidebar_position:6},l="Human Primacy",h={},o=[{value:"The Context Argument",id:"the-context-argument",level:2},{value:"Why &quot;Always Right&quot;?",id:"why-always-right",level:2},{value:"The Parent Analogy",id:"the-parent-analogy",level:3},{value:"Implications for AI Specialists",id:"implications-for-ai-specialists",level:2},{value:"1. Predict, Don&#39;t Judge",id:"1-predict-dont-judge",level:3},{value:"2. Judgment Criteria",id:"2-judgment-criteria",level:3},{value:"3. No Standing to Override",id:"3-no-standing-to-override",level:3},{value:"When Humans Disagree",id:"when-humans-disagree",level:2},{value:"Example: Two Reviewers Disagree",id:"example-two-reviewers-disagree",level:3},{value:"Practical Implementation",id:"practical-implementation",level:2},{value:"Weight System",id:"weight-system",level:3},{value:"Immediate Human Override",id:"immediate-human-override",level:3},{value:"Express Lane Trip Wire",id:"express-lane-trip-wire",level:3},{value:"Common Objections",id:"common-objections",level:2},{value:"&quot;But sometimes the AI is objectively right&quot;",id:"but-sometimes-the-ai-is-objectively-right",level:3},{value:"&quot;This slows down automation&quot;",id:"this-slows-down-automation",level:3},{value:"&quot;What about clear AI advantages (calculation, etc.)?&quot;",id:"what-about-clear-ai-advantages-calculation-etc",level:3},{value:"The Long Game",id:"the-long-game",level:2},{value:"Related Concepts",id:"related-concepts",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"human-primacy",children:"Human Primacy"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The human is always right"})," \u2014 not because humans are infallible, but because humans have context that AI cannot access."]}),"\n",(0,s.jsx)(n.p,{children:"This is the foundational principle of DIAL."}),"\n",(0,s.jsx)(n.h2,{id:"the-context-argument",children:"The Context Argument"}),"\n",(0,s.jsxs)(n.p,{children:["An AI model operates on a ",(0,s.jsx)(n.strong,{children:"bounded context window"}),"\u2014thousands or millions of tokens of visible information."]}),"\n",(0,s.jsx)(n.p,{children:"A human operates on:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["A ",(0,s.jsx)(n.strong,{children:"lifetime of embodied experience"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tacit knowledge"})," that can't be articulated"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Institutional context"})," and organizational history"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time sensory input"})," that no model can access"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relationships"})," and social dynamics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intuitions"})," built from millions of decisions"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The human knows things they ",(0,s.jsx)(n.strong,{children:"cannot tell the machine"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"why-always-right",children:'Why "Always Right"?'}),"\n",(0,s.jsxs)(n.p,{children:["This isn't a claim about human infallibility. Humans make mistakes constantly. The claim is about ",(0,s.jsx)(n.strong,{children:"information asymmetry"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"When a human's decision looks wrong from the AI's perspective, there are two possibilities:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"The human made an error"})," \u2014 possible, but the AI can't verify this"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"The human has context the AI doesn't"})," \u2014 invisible to the AI by definition"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The machine, trained on human works and operating on a compressed subset of human knowledge, ",(0,s.jsx)(n.strong,{children:"cannot determine when the human is wrong"}),"\u2014because what looks like an error from the AI's limited vantage point may reflect context the AI simply doesn't have."]}),"\n",(0,s.jsx)(n.h3,{id:"the-parent-analogy",children:"The Parent Analogy"}),"\n",(0,s.jsx)(n.p,{children:"It is always safer for the AI to assume the human had reasons, just as it is safer for a child to defer to a parent\u2014not because the parent is infallible, but because the parent has context the child cannot access."}),"\n",(0,s.jsx)(n.p,{children:"The child might think:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"Why can\'t I have ice cream for dinner?"'}),"\n",(0,s.jsx)(n.li,{children:'"Why do I have to go to bed now?"'}),"\n",(0,s.jsx)(n.li,{children:'"Why can\'t I play in the street?"'}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The parent has context about nutrition, sleep needs, and traffic that the child can't fully grasp. The child should defer even when they disagree, because the parent's broader context makes the parent's decision the more reliable one."}),"\n",(0,s.jsx)(n.p,{children:"AI specialists should adopt the same posture."}),"\n",(0,s.jsx)(n.h2,{id:"implications-for-ai-specialists",children:"Implications for AI Specialists"}),"\n",(0,s.jsx)(n.h3,{id:"1-predict-dont-judge",children:"1. Predict, Don't Judge"}),"\n",(0,s.jsxs)(n.p,{children:["An AI specialist should choose what the human ",(0,s.jsx)(n.strong,{children:"would"})," choose, even if its own reasoning disagrees."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'\u274c "Based on my analysis, the correct action is X"\n\u2705 "Based on observed human patterns, the human would likely choose Y"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-judgment-criteria",children:"2. Judgment Criteria"}),"\n",(0,s.jsxs)(n.p,{children:["AI specialists are judged on ",(0,s.jsx)(n.strong,{children:"alignment with human choices"}),", not on their independent correctness:"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Metric"}),(0,s.jsx)(n.th,{children:"Good"}),(0,s.jsx)(n.th,{children:"Bad"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Alignment rate"}),(0,s.jsx)(n.td,{children:"95% match with human"}),(0,s.jsx)(n.td,{children:"60% match with human"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Reasoning quality"}),(0,s.jsx)(n.td,{children:'"Human would prefer X because..."'}),(0,s.jsx)(n.td,{children:'"The objectively correct answer is..."'})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Confidence calibration"}),(0,s.jsx)(n.td,{children:'"High confidence human chooses X"'}),(0,s.jsx)(n.td,{children:'"I am certain X is correct"'})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"3-no-standing-to-override",children:"3. No Standing to Override"}),"\n",(0,s.jsx)(n.p,{children:"If an AI specialist has strong reasoning that the human is wrong, it should:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 Present its reasoning in the proposal"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Let the human see and consider it"}),"\n",(0,s.jsx)(n.li,{children:"\u274c NOT override the human decision"}),"\n",(0,s.jsx)(n.li,{children:"\u274c NOT claim authority based on its reasoning"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"when-humans-disagree",children:"When Humans Disagree"}),"\n",(0,s.jsx)(n.p,{children:"If the human is always right, and there's more than one human, then humans can disagree\u2014but even in disagreement, they are both right compared to an AI."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"An AI has no standing to break the tie."})}),"\n",(0,s.jsx)(n.p,{children:"Human disagreement is resolved by human mechanisms:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Negotiation"}),"\n",(0,s.jsx)(n.li,{children:"Authority structures"}),"\n",(0,s.jsx)(n.li,{children:"Voting among humans"}),"\n",(0,s.jsx)(n.li,{children:"Escalation to decision-makers"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The AI's role is to ",(0,s.jsx)(n.strong,{children:"predict what the humans would collectively choose"}),", not to adjudicate between them."]}),"\n",(0,s.jsx)(n.h3,{id:"example-two-reviewers-disagree",children:"Example: Two Reviewers Disagree"}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TD\n    P[Proposal: Approve PR]\n    R1[Reviewer A: Approve]\n    R2[Reviewer B: Request Changes]\n    AI[AI Specialist]\n    \n    R1 --\x3e|"Good code"| P\n    R2 --\x3e|"Needs tests"| P\n    AI --\x3e|"Cannot break tie"| P'}),"\n",(0,s.jsx)(n.p,{children:"The AI might have an opinion about whether tests are needed. It doesn't matter. The AI:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Reports both human preferences accurately"}),"\n",(0,s.jsx)(n.li,{children:"May note the disagreement exists"}),"\n",(0,s.jsx)(n.li,{children:"Does NOT cast a deciding vote"}),"\n",(0,s.jsx)(n.li,{children:"Defers to whatever human mechanism resolves disputes (e.g., senior reviewer, author decides, etc.)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"practical-implementation",children:"Practical Implementation"}),"\n",(0,s.jsx)(n.h3,{id:"weight-system",children:"Weight System"}),"\n",(0,s.jsx)(n.p,{children:"DIAL implements human primacy through the weight system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// Starting weights\nhumanSpecialist.weight = 1.0;  // Full authority\naiSpecialist.weight = 0.0;     // No authority\n\n// Arbitration behavior\nif (humanVoted) {\n  return humanChoice;  // Human wins immediately\n}\n// Only if no human voted:\nreturn weightedConsensus(aiVotes);\n"})}),"\n",(0,s.jsx)(n.h3,{id:"immediate-human-override",children:"Immediate Human Override"}),"\n",(0,s.jsx)(n.p,{children:"Any human vote immediately wins, regardless of how many AI specialists disagree:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"AI Specialist 1: Approve (weight 0.8)\nAI Specialist 2: Approve (weight 0.6)\nAI Specialist 3: Approve (weight 0.7)\nHuman: Request Changes (weight 1.0)\n\nResult: Request Changes \u2713\n"})}),"\n",(0,s.jsx)(n.h3,{id:"express-lane-trip-wire",children:"Express Lane Trip Wire"}),"\n",(0,s.jsx)(n.p,{children:'Even in "express lane" mode (high-confidence automation), human intervention immediately drops the system back to full deliberation:'}),"\n",(0,s.jsx)(n.mermaid,{value:'graph LR\n    E[Express Lane] --\x3e|"Human Override"| F[Full Deliberation]\n    F --\x3e|"Rebuild Confidence"| E'}),"\n",(0,s.jsx)(n.h2,{id:"common-objections",children:"Common Objections"}),"\n",(0,s.jsx)(n.h3,{id:"but-sometimes-the-ai-is-objectively-right",children:'"But sometimes the AI is objectively right"'}),"\n",(0,s.jsx)(n.p,{children:'Define "objectively." From whose perspective? With what information?'}),"\n",(0,s.jsx)(n.p,{children:'The AI operates on a subset of reality. When it seems "objectively right," that assessment is made from within its limited context. The human may have information that changes the entire picture.'}),"\n",(0,s.jsx)(n.h3,{id:"this-slows-down-automation",children:'"This slows down automation"'}),"\n",(0,s.jsx)(n.p,{children:"Yes, initially. But DIAL's progressive collapse means that as AI proves alignment with human judgment, the system naturally automates. Human primacy ensures this automation is earned, not assumed."}),"\n",(0,s.jsx)(n.h3,{id:"what-about-clear-ai-advantages-calculation-etc",children:'"What about clear AI advantages (calculation, etc.)?"'}),"\n",(0,s.jsxs)(n.p,{children:["For tasks where AI has clear advantages (arithmetic, data lookup, pattern matching on defined criteria), those become ",(0,s.jsx)(n.strong,{children:"Tool specialists"})," with deterministic execution\u2014no voting needed."]}),"\n",(0,s.jsxs)(n.p,{children:["Human primacy applies to ",(0,s.jsx)(n.strong,{children:"judgment calls"}),", not computation."]}),"\n",(0,s.jsx)(n.h2,{id:"the-long-game",children:"The Long Game"}),"\n",(0,s.jsxs)(n.p,{children:["Human primacy is not a limitation on AI\u2014it's the foundation for ",(0,s.jsx)(n.strong,{children:"trustworthy AI integration"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"By measuring alignment with human judgment across thousands of decisions, organizations can:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Discover"})," which decisions AI handles well"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quantify"})," the cost of human oversight"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Calibrate"})," automation to actual capability"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Build confidence"})," through demonstrated performance"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:'The alternative\u2014assuming AI capability and letting it run\u2014produces "AI failures" that erode trust and invite regulation.'}),"\n",(0,s.jsx)(n.p,{children:"Human primacy is the sustainable path to AI adoption."}),"\n",(0,s.jsx)(n.h2,{id:"related-concepts",children:"Related Concepts"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/dialai/docs/concepts/specialists#weight-and-trust",children:"Empirical Trust"})," \u2014 How AI earns authority"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/dialai/docs/concepts/arbitration",children:"Arbitration"})," \u2014 Consensus mechanisms"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/dialai/docs/concepts/decision-cycle",children:"Decision Cycle"})," \u2014 The process that implements human primacy"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);