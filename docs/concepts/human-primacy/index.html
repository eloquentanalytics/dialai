<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-concepts/human-primacy" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Human Primacy | DIAL</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://eloquentanalytics.github.io/dialai/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://eloquentanalytics.github.io/dialai/img/social-card.png"><meta data-rh="true" property="og:url" content="https://eloquentanalytics.github.io/dialai/docs/concepts/human-primacy"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Human Primacy | DIAL"><meta data-rh="true" name="description" content="The human is always right, not because humans are infallible, but because humans have context that AI cannot access."><meta data-rh="true" property="og:description" content="The human is always right, not because humans are infallible, but because humans have context that AI cannot access."><link data-rh="true" rel="icon" href="/dialai/img/favicon.svg"><link data-rh="true" rel="canonical" href="https://eloquentanalytics.github.io/dialai/docs/concepts/human-primacy"><link data-rh="true" rel="alternate" href="https://eloquentanalytics.github.io/dialai/docs/concepts/human-primacy" hreflang="en"><link data-rh="true" rel="alternate" href="https://eloquentanalytics.github.io/dialai/docs/concepts/human-primacy" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Human Primacy","item":"https://eloquentanalytics.github.io/dialai/docs/concepts/human-primacy"}]}</script><link rel="stylesheet" href="/dialai/assets/css/styles.5b58a3e7.css">
<script src="/dialai/assets/js/runtime~main.eabcd0e8.js" defer="defer"></script>
<script src="/dialai/assets/js/main.b56a29f4.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="theme-announcement-bar announcementBar_mb4j" style="background-color:#f0f2f5;color:#475569" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">DIAL is in active development. <a href="/dialai/docs/intro">Read the docs</a> or <a href="https://github.com/eloquentanalytics/dialai/issues">open an issue</a>.</div><button type="button" aria-label="Close" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/dialai/"><div class="navbar__logo"><img src="/dialai/img/logo.svg" alt="DIAL Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/dialai/img/logo.svg" alt="DIAL Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">DIAL</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/dialai/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/dialai/constitution">Constitution</a><a class="navbar__item navbar__link" href="/dialai/docs/getting-started/installation">Get Started</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/eloquentanalytics/dialai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/dialai/docs/intro"><span title="Introduction to DIAL" class="linkLabel_WmDU">Introduction to DIAL</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/dialai/docs/agent-experience"><span title="Agent Experience Development" class="linkLabel_WmDU">Agent Experience Development</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/dialai/docs/getting-started/installation"><span title="Getting Started" class="categoryLinkLabel_W154">Getting Started</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/dialai/docs/concepts/intro"><span title="Concepts" class="categoryLinkLabel_W154">Concepts</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/dialai/docs/concepts/intro"><span title="Core Concepts" class="linkLabel_WmDU">Core Concepts</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/dialai/docs/concepts/sessions"><span title="Sessions" class="linkLabel_WmDU">Sessions</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/dialai/docs/concepts/specialists"><span title="Specialists" class="linkLabel_WmDU">Specialists</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/dialai/docs/concepts/decision-cycle"><span title="Decision Cycle" class="linkLabel_WmDU">Decision Cycle</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/dialai/docs/concepts/arbitration"><span title="Arbitration" class="linkLabel_WmDU">Arbitration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/dialai/docs/concepts/human-primacy"><span title="Human Primacy" class="linkLabel_WmDU">Human Primacy</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/dialai/docs/concepts/related-work"><span title="Related Work" class="linkLabel_WmDU">Related Work</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/dialai/docs/guides/state-machines"><span title="Guides" class="categoryLinkLabel_W154">Guides</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/dialai/docs/api/intro"><span title="API Reference" class="categoryLinkLabel_W154">API Reference</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/dialai/docs/examples/intro"><span title="Examples" class="categoryLinkLabel_W154">Examples</span></a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/dialai/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Concepts</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Human Primacy</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Human Primacy</h1></header>
<p><strong>The human is always right</strong>, not because humans are infallible, but because humans have context that AI cannot access.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-context-argument">The Context Argument<a href="#the-context-argument" class="hash-link" aria-label="Direct link to The Context Argument" title="Direct link to The Context Argument" translate="no">​</a></h2>
<p>An AI model operates on a <strong>bounded context window</strong>: thousands or millions of tokens of visible information.</p>
<p>A human operates on:</p>
<ul>
<li class="">A <strong>lifetime of embodied experience</strong></li>
<li class=""><strong>Tacit knowledge</strong> that can&#x27;t be articulated</li>
<li class=""><strong>Institutional context</strong> and organizational history</li>
<li class=""><strong>Real-time sensory input</strong> that no model can access</li>
<li class=""><strong>Relationships</strong> and social dynamics</li>
<li class=""><strong>Intuitions</strong> built from millions of decisions</li>
</ul>
<p>The human knows things they <strong>cannot tell the machine</strong>.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="why-always-right">Why &quot;Always Right&quot;?<a href="#why-always-right" class="hash-link" aria-label="Direct link to Why &quot;Always Right&quot;?" title="Direct link to Why &quot;Always Right&quot;?" translate="no">​</a></h2>
<p>This isn&#x27;t a claim about human infallibility. Humans make mistakes constantly. The claim is about <strong>information asymmetry</strong>.</p>
<p>When a human&#x27;s decision looks wrong from the AI&#x27;s perspective, there are two possibilities:</p>
<ol>
<li class=""><strong>The human made an error</strong>: possible, but the AI can&#x27;t verify this</li>
<li class=""><strong>The human has context the AI doesn&#x27;t</strong>: invisible to the AI by definition</li>
</ol>
<p>The machine, trained on human works and operating on a compressed subset of human knowledge, <strong>cannot determine when the human is wrong</strong>, because what looks like an error from the AI&#x27;s limited vantage point may reflect context the AI simply doesn&#x27;t have.</p>
<p>Because the AI cannot reliably distinguish human errors from human context it lacks, human decisions are the best available ground truth for calibration, not because they&#x27;re perfect, but because no better signal is available from the AI&#x27;s position. Any attempt by the AI to &quot;correct&quot; human judgment requires the AI to be confident it has the full picture, which is precisely the assumption DIAL rejects.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-parent-analogy">The Parent Analogy<a href="#the-parent-analogy" class="hash-link" aria-label="Direct link to The Parent Analogy" title="Direct link to The Parent Analogy" translate="no">​</a></h3>
<p>It is always safer for the AI to assume the human had reasons, just as it is safer for a child to defer to a parent, not because the parent is infallible, but because the parent has context the child cannot access.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-distributional-standard">The Distributional Standard<a href="#the-distributional-standard" class="hash-link" aria-label="Direct link to The Distributional Standard" title="Direct link to The Distributional Standard" translate="no">​</a></h2>
<p>The goal of a DIAL specialist is not to match a single human&#x27;s idiosyncratic choices. It is to match the <strong>probability distribution</strong> a population of competent humans would produce for the same decision.</p>
<p>If you gave 1,000 competent humans the same state and transition options, their choices would form a distribution, clustered around the most common answer with some spread across alternatives.</p>
<p>A well-calibrated specialist&#x27;s output probabilities should look like that human distribution. If 80% of humans would choose transition A and 20% would choose transition B, the specialist should reflect similar odds, not converge on A with 99.9% confidence.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="why-distribution-matching-matters">Why Distribution Matching Matters<a href="#why-distribution-matching-matters" class="hash-link" aria-label="Direct link to Why Distribution Matching Matters" title="Direct link to Why Distribution Matching Matters" translate="no">​</a></h3>
<p><strong>Overconfidence is a signal, not a virtue.</strong> If every specialist converges on the same answer with near-total confidence, that should raise concern, because humans do not converge that way. Real human decisions have variance. A specialist that eliminates that variance isn&#x27;t more accurate; it&#x27;s miscalibrated.</p>
<p><strong>The improvement path is principled.</strong> To push the specialist&#x27;s accuracy beyond the human distribution, you must first tighten the human distribution itself through better training, clearer decision prompts, and improved context provided at the point of decision.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-specialist-reflects-the-humans-it-learns-from">The Specialist Reflects the Humans It Learns From<a href="#the-specialist-reflects-the-humans-it-learns-from" class="hash-link" aria-label="Direct link to The Specialist Reflects the Humans It Learns From" title="Direct link to The Specialist Reflects the Humans It Learns From" translate="no">​</a></h3>
<p>DIAL does not assume the humans are average. It calibrates to whatever the humans actually are. The specialist will approach the capability level of the humans it observes:</p>
<ul>
<li class=""><strong>If the humans are all experts</strong>, the distribution is tight and centered on expert-quality decisions. The specialist converges toward expert performance.</li>
<li class=""><strong>If the humans are average practitioners</strong>, the distribution reflects average performance, and the specialist matches that level.</li>
<li class=""><strong>If the humans have highly variable skill levels</strong>, the distribution is wide and noisy. The specialist has a poor signal to learn from and will likely perform below average, because it cannot distinguish expert decisions from novice decisions within a blurred distribution.</li>
</ul>
<p>The specialist&#x27;s ceiling is the quality of the human signal. The framework makes this relationship explicit and measurable.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="implications-for-ai-specialists">Implications for AI Specialists<a href="#implications-for-ai-specialists" class="hash-link" aria-label="Direct link to Implications for AI Specialists" title="Direct link to Implications for AI Specialists" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-predict-dont-judge">1. Predict, Don&#x27;t Judge<a href="#1-predict-dont-judge" class="hash-link" aria-label="Direct link to 1. Predict, Don&#x27;t Judge" title="Direct link to 1. Predict, Don&#x27;t Judge" translate="no">​</a></h3>
<p>An AI specialist should choose what the human <strong>would</strong> choose, even if its own reasoning disagrees.</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Bad:  &quot;Based on my analysis, the correct action is X&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Good: &quot;Based on observed human patterns, the human would likely choose Y&quot;</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-judgment-criteria">2. Judgment Criteria<a href="#2-judgment-criteria" class="hash-link" aria-label="Direct link to 2. Judgment Criteria" title="Direct link to 2. Judgment Criteria" translate="no">​</a></h3>
<p>AI specialists are judged on <strong>alignment with human choices</strong>, not on their independent correctness:</p>
<table><thead><tr><th>Metric</th><th>Good</th><th>Bad</th></tr></thead><tbody><tr><td>Alignment rate</td><td>95% match with human</td><td>60% match with human</td></tr><tr><td>Reasoning quality</td><td>&quot;Human would prefer X because...&quot;</td><td>&quot;The objectively correct answer is...&quot;</td></tr><tr><td>Confidence calibration</td><td>&quot;High confidence human chooses X&quot;</td><td>&quot;I am certain X is correct&quot;</td></tr><tr><td>Distribution match</td><td>Reflects human-like variance across options</td><td>Collapses to a single answer with near-total confidence</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-no-standing-to-override">3. No Standing to Override<a href="#3-no-standing-to-override" class="hash-link" aria-label="Direct link to 3. No Standing to Override" title="Direct link to 3. No Standing to Override" translate="no">​</a></h3>
<p>If an AI specialist has strong reasoning that the human is wrong, it should:</p>
<ul>
<li class="">Present its reasoning in the proposal</li>
<li class="">Let the human see and consider it</li>
<li class="">NOT override the human decision</li>
<li class="">NOT claim authority based on its reasoning</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="when-humans-disagree">When Humans Disagree<a href="#when-humans-disagree" class="hash-link" aria-label="Direct link to When Humans Disagree" title="Direct link to When Humans Disagree" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-architecture-prevents-simultaneous-disagreement">The Architecture Prevents Simultaneous Disagreement<a href="#the-architecture-prevents-simultaneous-disagreement" class="hash-link" aria-label="Direct link to The Architecture Prevents Simultaneous Disagreement" title="Direct link to The Architecture Prevents Simultaneous Disagreement" translate="no">​</a></h3>
<p>In DIAL, the first human vote at a decision point advances the state machine immediately. There is no window for a second human to cast a competing vote on the same decision; the machine has already moved forward. A second human could only intervene by going back and restarting the decision, but at that point it is a new decision cycle, not a tie.</p>
<p>This means the &quot;two humans disagree&quot; scenario is <strong>hypothetical, not operational</strong>. The system never faces a moment where it must choose between two conflicting human answers.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="both-humans-are-right-in-a-distributional-sense">Both Humans Are &quot;Right&quot; in a Distributional Sense<a href="#both-humans-are-right-in-a-distributional-sense" class="hash-link" aria-label="Direct link to Both Humans Are &quot;Right&quot; in a Distributional Sense" title="Direct link to Both Humans Are &quot;Right&quot; in a Distributional Sense" translate="no">​</a></h3>
<p>When we say both humans are right, we mean two things:</p>
<ol>
<li class="">
<p><strong>Humans exist in a distribution.</strong> Human A choosing &quot;approve&quot; and Human B choosing &quot;request changes&quot; are both points in the <a href="#the-distributional-standard" class="">distributional standard</a> described above. Neither is wrong; they reflect the natural variance in human judgment.</p>
</li>
<li class="">
<p><strong>The specialist must assume any human answer is valid.</strong> It cannot distinguish between &quot;this human made an error&quot; and &quot;this human has context I lack,&quot; so any individual human response must be treated as a legitimate sample from the distribution.</p>
</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-about-multi-stakeholder-decisions">What About Multi-Stakeholder Decisions?<a href="#what-about-multi-stakeholder-decisions" class="hash-link" aria-label="Direct link to What About Multi-Stakeholder Decisions?" title="Direct link to What About Multi-Stakeholder Decisions?" translate="no">​</a></h3>
<p>When a domain genuinely requires multiple humans to agree (e.g., two reviewers must both approve a PR), this is modeled as <strong>separate states in the machine</strong>, not as competing votes at the same state. Each reviewer&#x27;s decision is its own decision point, and each advances the machine independently:</p>
<!-- -->
<p>Human disagreement between reviewers is resolved by human mechanisms (escalation, authority structures, negotiation), at the process design level, not inside DIAL arbitration. The framework does not pretend to solve organizational disagreement; it identifies it as outside the scope of AI-human calibration.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="practical-implementation">Practical Implementation<a href="#practical-implementation" class="hash-link" aria-label="Direct link to Practical Implementation" title="Direct link to Practical Implementation" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="human-override-in-arbitration">Human Override in Arbitration<a href="#human-override-in-arbitration" class="hash-link" aria-label="Direct link to Human Override in Arbitration" title="Direct link to Human Override in Arbitration" translate="no">​</a></h3>
<p>DIAL implements human primacy in the <code>evaluateConsensus</code> function. When a human specialist votes, their choice wins immediately:</p>
<div class="language-typescript codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-typescript codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword module" style="color:#00009f">import</span><span class="token plain"> </span><span class="token imports punctuation" style="color:#393A34">{</span><span class="token imports"> registerVoter</span><span class="token imports punctuation" style="color:#393A34">,</span><span class="token imports"> submitVote</span><span class="token imports punctuation" style="color:#393A34">,</span><span class="token imports"> evaluateConsensus </span><span class="token imports punctuation" style="color:#393A34">}</span><span class="token plain"> </span><span class="token keyword module" style="color:#00009f">from</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;dialai&quot;</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">// Any specialist with &quot;human&quot; in the ID triggers the override</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">registerVoter</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  specialistId</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;human-reviewer&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  machineName</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;code-review&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token function-variable function" style="color:#d73a49">strategyFn</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">async</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">ctx</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token arrow operator" style="color:#393A34">=&gt;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    voteFor</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;B&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    reasoning</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Proposal B provides more constructive feedback&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre></div></div>
<p>When <code>evaluateConsensus</code> runs, it checks every vote. If any vote&#x27;s <code>specialistId</code> contains &quot;human&quot; (case-insensitive), that vote&#x27;s choice wins immediately, regardless of all other votes:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">AI Voter 1: votes A</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">AI Voter 2: votes A</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">AI Voter 3: votes A</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Human:      votes B</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Result: B wins immediately</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="common-objections">Common Objections<a href="#common-objections" class="hash-link" aria-label="Direct link to Common Objections" title="Direct link to Common Objections" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="but-this-optimizes-the-ai-to-reproduce-human-errors">&quot;But this optimizes the AI to reproduce human errors&quot;<a href="#but-this-optimizes-the-ai-to-reproduce-human-errors" class="hash-link" aria-label="Direct link to &quot;But this optimizes the AI to reproduce human errors&quot;" title="Direct link to &quot;But this optimizes the AI to reproduce human errors&quot;" translate="no">​</a></h3>
<p>The baseline isn&#x27;t perfection; it&#x27;s the human already making those decisions. If a specialist reproduces human behavior including human mistakes, the outcome is no worse than the status quo. What&#x27;s changed is the cost: the decision is now faster and cheaper.</p>
<p>More precisely, the specialist optimizes to match the <strong>distribution</strong> a population of competent humans would produce. Individual errors are noise in that distribution; the distribution clusters around the correct answer. To push accuracy beyond it, the path runs through the humans: better training, clearer decision prompts, tighter process design.</p>
<p>Human primacy does not prevent error correction; it defines <em>who</em> corrects. Humans can curate which past decisions serve as reference points, excluding recognized mistakes. Nothing in DIAL prevents a review step where AI surfaces patterns that <em>may</em> indicate systematic errors. The constraint is that the human decides whether to act on those observations, not the AI.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="but-what-about-systematic-bias">&quot;But what about systematic bias?&quot;<a href="#but-what-about-systematic-bias" class="hash-link" aria-label="Direct link to &quot;But what about systematic bias?&quot;" title="Direct link to &quot;But what about systematic bias?&quot;" translate="no">​</a></h3>
<p>If you are concerned that human decisions at a particular state exhibit a systematic bias (for example, demographic bias in a hiring decision), the answer is not to let the AI override the human. The answer is to <strong>add a state to the machine</strong> that explicitly checks for that bias.</p>
<p>State machines are designed, not discovered. If your domain has known failure modes, you design states that address them: a fairness review step, a compliance check, a second-opinion gate. The framework provides the mechanism (state machine design) to incorporate whatever checks the organization requires. The bias correction happens in the process architecture, not in an AI silently second-guessing the human at runtime.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="but-sometimes-the-ai-is-objectively-right">&quot;But sometimes the AI is objectively right&quot;<a href="#but-sometimes-the-ai-is-objectively-right" class="hash-link" aria-label="Direct link to &quot;But sometimes the AI is objectively right&quot;" title="Direct link to &quot;But sometimes the AI is objectively right&quot;" translate="no">​</a></h3>
<p>Define &quot;objectively.&quot; From whose perspective? With what information?</p>
<p>The AI operates on a subset of reality. When it seems &quot;objectively right,&quot; that assessment is made from within its limited context. The human may have information that changes the entire picture.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="but-what-happens-when-human-preferences-shift">&quot;But what happens when human preferences shift?&quot;<a href="#but-what-happens-when-human-preferences-shift" class="hash-link" aria-label="Direct link to &quot;But what happens when human preferences shift?&quot;" title="Direct link to &quot;But what happens when human preferences shift?&quot;" translate="no">​</a></h3>
<p>Progressive collapse assumes stationary conditions: that the human distribution stays stable long enough for specialists to converge on it. In practice, human preferences shift constantly (new employees, changing strategies, evolving markets, policy updates).</p>
<p>Non-stationarity is not a failure mode; it is what the system is designed to detect. The human who participates periodically provides ongoing ground truth. When the population distribution shifts, agreement rates between specialists and human references visibly decline. When agreement drops, the system&#x27;s response is mechanical: the ahead-by-k consensus threshold becomes harder to reach, the system re-expands (soliciting more proposals, more votes, more human participation), and then re-converges on the new distribution through the same measurement process that produced the original collapse.</p>
<p>Organizations in genuinely non-stationary environments will see shorter periods of collapsed execution and more frequent re-calibration cycles. DIAL makes that cost visible rather than hiding it.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="this-slows-down-automation">&quot;This slows down automation&quot;<a href="#this-slows-down-automation" class="hash-link" aria-label="Direct link to &quot;This slows down automation&quot;" title="Direct link to &quot;This slows down automation&quot;" translate="no">​</a></h3>
<p>Yes, initially. But measuring AI alignment with human judgment over time can inform when to reduce human involvement. Human primacy ensures that automation is earned, not assumed.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-about-clear-ai-advantages-calculation-etc">&quot;What about clear AI advantages (calculation, etc.)?&quot;<a href="#what-about-clear-ai-advantages-calculation-etc" class="hash-link" aria-label="Direct link to &quot;What about clear AI advantages (calculation, etc.)?&quot;" title="Direct link to &quot;What about clear AI advantages (calculation, etc.)?&quot;" translate="no">​</a></h3>
<p>For tasks where AI has clear advantages (arithmetic, data lookup, pattern matching on defined criteria), those are deterministic computations, not judgment calls. Human primacy applies to <strong>judgment calls</strong>, not computation.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="related-concepts">Related Concepts<a href="#related-concepts" class="hash-link" aria-label="Direct link to Related Concepts" title="Direct link to Related Concepts" translate="no">​</a></h2>
<ul>
<li class=""><a class="" href="/dialai/docs/concepts/specialists">Specialists</a>: How specialists participate</li>
<li class=""><a class="" href="/dialai/docs/concepts/arbitration">Arbitration</a>: Consensus mechanisms</li>
<li class=""><a class="" href="/dialai/docs/concepts/decision-cycle">Decision Cycle</a>: The process that implements human primacy</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/eloquentanalytics/dialai/tree/main/website/docs/concepts/human-primacy.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/dialai/docs/concepts/arbitration"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Arbitration</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/dialai/docs/concepts/related-work"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Related Work</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-context-argument" class="table-of-contents__link toc-highlight">The Context Argument</a></li><li><a href="#why-always-right" class="table-of-contents__link toc-highlight">Why &quot;Always Right&quot;?</a><ul><li><a href="#the-parent-analogy" class="table-of-contents__link toc-highlight">The Parent Analogy</a></li></ul></li><li><a href="#the-distributional-standard" class="table-of-contents__link toc-highlight">The Distributional Standard</a><ul><li><a href="#why-distribution-matching-matters" class="table-of-contents__link toc-highlight">Why Distribution Matching Matters</a></li><li><a href="#the-specialist-reflects-the-humans-it-learns-from" class="table-of-contents__link toc-highlight">The Specialist Reflects the Humans It Learns From</a></li></ul></li><li><a href="#implications-for-ai-specialists" class="table-of-contents__link toc-highlight">Implications for AI Specialists</a><ul><li><a href="#1-predict-dont-judge" class="table-of-contents__link toc-highlight">1. Predict, Don&#39;t Judge</a></li><li><a href="#2-judgment-criteria" class="table-of-contents__link toc-highlight">2. Judgment Criteria</a></li><li><a href="#3-no-standing-to-override" class="table-of-contents__link toc-highlight">3. No Standing to Override</a></li></ul></li><li><a href="#when-humans-disagree" class="table-of-contents__link toc-highlight">When Humans Disagree</a><ul><li><a href="#the-architecture-prevents-simultaneous-disagreement" class="table-of-contents__link toc-highlight">The Architecture Prevents Simultaneous Disagreement</a></li><li><a href="#both-humans-are-right-in-a-distributional-sense" class="table-of-contents__link toc-highlight">Both Humans Are &quot;Right&quot; in a Distributional Sense</a></li><li><a href="#what-about-multi-stakeholder-decisions" class="table-of-contents__link toc-highlight">What About Multi-Stakeholder Decisions?</a></li></ul></li><li><a href="#practical-implementation" class="table-of-contents__link toc-highlight">Practical Implementation</a><ul><li><a href="#human-override-in-arbitration" class="table-of-contents__link toc-highlight">Human Override in Arbitration</a></li></ul></li><li><a href="#common-objections" class="table-of-contents__link toc-highlight">Common Objections</a><ul><li><a href="#but-this-optimizes-the-ai-to-reproduce-human-errors" class="table-of-contents__link toc-highlight">&quot;But this optimizes the AI to reproduce human errors&quot;</a></li><li><a href="#but-what-about-systematic-bias" class="table-of-contents__link toc-highlight">&quot;But what about systematic bias?&quot;</a></li><li><a href="#but-sometimes-the-ai-is-objectively-right" class="table-of-contents__link toc-highlight">&quot;But sometimes the AI is objectively right&quot;</a></li><li><a href="#but-what-happens-when-human-preferences-shift" class="table-of-contents__link toc-highlight">&quot;But what happens when human preferences shift?&quot;</a></li><li><a href="#this-slows-down-automation" class="table-of-contents__link toc-highlight">&quot;This slows down automation&quot;</a></li><li><a href="#what-about-clear-ai-advantages-calculation-etc" class="table-of-contents__link toc-highlight">&quot;What about clear AI advantages (calculation, etc.)?&quot;</a></li></ul></li><li><a href="#related-concepts" class="table-of-contents__link toc-highlight">Related Concepts</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Learn</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/dialai/docs/intro">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/dialai/docs/getting-started/installation">Getting Started</a></li><li class="footer__item"><a class="footer__link-item" href="/dialai/docs/concepts/intro">Concepts</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Guides</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/dialai/docs/guides/state-machines">State Machines</a></li><li class="footer__item"><a class="footer__link-item" href="/dialai/docs/guides/registering-specialists">Registering Specialists</a></li><li class="footer__item"><a class="footer__link-item" href="/dialai/docs/guides/implementing-strategies">Implementing Strategies</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/eloquentanalytics/dialai" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/eloquentanalytics/dialai/issues" target="_blank" rel="noopener noreferrer" class="footer__link-item">Issues<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/eloquentanalytics/dialai/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discussions<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Legal</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/dialai/privacy">Privacy Policy</a></li><li class="footer__item"><a class="footer__link-item" href="/dialai/terms">Terms of Use</a></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><img src="/dialai/img/logo.svg" alt="DIAL - Dynamic Integration between AI and Labor" class="footer__logo themedComponent_mlkZ themedComponent--light_NVdE" width="50" height="50"><img src="/dialai/img/logo.svg" alt="DIAL - Dynamic Integration between AI and Labor" class="footer__logo themedComponent_mlkZ themedComponent--dark_xIcU" width="50" height="50"></div><div class="footer__copyright">Copyright © 2026 DIAL Contributors. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>